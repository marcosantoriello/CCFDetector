\documentclass[]{article}
\usepackage{fontspec}
%\setmainfont{Arial}[ItalicFont={Arial Italic}]
%\setmainfont{Gill Sans MT}[ItalicFont={Gill Sans MT Italic}]
\usepackage[utf8]{inputenc}
\usepackage[margin=1.5cm, bindingoffset=1cm]{geometry}
\linespread{1.5}
\usepackage{float}
\usepackage{csquotes}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{indentfirst}
\setlength{\parindent}{0cm}
\usepackage[italian]{babel}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumerate}
% Imposto colore hyperlinks
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    }
\usepackage{color}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{url}

\lstset{showstringspaces=false}

\title{CCFDetector}
\author{Marco Santoriello}
\date{Gennaio 2024}

\begin{document}
\begin{titlepage}
    \begin{center}
        \LARGE{\uppercase{Università degli Studi di Salerno}}\\
        \vspace{5mm}
        %Dipartimento
    	\uppercase{\normalsize Dipartimento di Informatica}\\
    \end{center}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.35\textwidth]{img/logo_unisa.png}
    \end{figure}

    \begin{center}
        %Corso di Laurea
    	\normalsize{ Corso di Laurea in informatica }\\
    	\vspace{15mm}
    	%Titolo
        {\LARGE{\bf CCFDetector: Using ML against Credit Card Frauds }}\\
        {\large{ Progetto realizzato per l'esame di Fondamenti di Intelligenza Artificiale}}\\
    	\vspace{10mm}
    \end{center}
    \begin{minipage}[t]{0.4\textwidth}\raggedright
        %Candidato
    	{\large{Scritto da: \\ \bf Marco Santoriello\\ Mat. 0512114100}}
    \end{minipage}

    \vspace{90mm}
    %Anno Accademico
    \centering{\large \uppercase{ Anno Accademico 2023/2024 }}

\end{titlepage}

\setcounter{tocdepth}{3} %IMPOSTO LIVELLO PROFONDITA' INDICE

\tableofcontents
\newpage


\section{Introduzione}
    Negli ultimi anni, sempre più piede hanno preso i pagamenti elettronici, al punto che, anche in Italia, per legge, ogni commerciante deve essere munito di un dispositivo che permetta al cliente di pagare utilizzando la propria carta di credito, rischiando, in caso di mancato adempimento a questa legge, ingenti sanzioni pecuniarie.\\
    Come è facile immaginare, questo cambiamento nel modo in cui il denaro viene messo in circolazione ha interessato notevolmente criminali e truffatori (i cosiddetti \textit{scammers}), i quali hanno trovato non pochi modi di impossessarsi illecitamente, in maniera fisica o meno, delle carte di credito altrui.
    Basti pensare che negli Stati Uniti, secondo la \textit{Federal Trade Commission}, la tipologia di furti di identità più diffusi è proprio correlata alle frodi relative alle carte di credito.\\
    Queste frodi possono avvenire in svariati modi: si parte dal furto vero e proprio della carta di credito, fino ad arrivare, tramite diversi metodi, all'appropriazione dei soli dati della carta che abilitano al pagamento, passando per la clonazione delle carte e per l'utilizzo di dispositivi \textit{contactless} in posti affollati in prossimità dei portafogli delle ignare vittime.\\
    Questo progetto nasce con lo scopo di addestrare un modello di Machine Learning che permetta una facile ed affidabile individuazione delle transazioni fraudolente.

    \subsection{Specifica PEAS}
        La specifica PEAS (Performance, Environment, Actuators, Sensors) è un sistema che permette di descrivere l'ambiente operativo di un agente intelligente. L'obiettivo principale del progetto è quello di massimizzare la capacità dell'agente di rilevare transazioni fraudolente. L'ambiente in cui l'agente dovrà operare è di seguito descritto:
        \begin{itemize}
            \item Performance:
            \item Environment:
            \item Actuators:
            \item Sensors:
        \end{itemize}

    \subsection{Caratteristiche dell'ambiente}
        % TO-DO

    \subsection{Analisi del Problema}
        %TO-DO

\section{Data Understanding}
    \subsection{Data Collection}
        Definito il Problem Statement, passo all'individuazione del dataset adatto per l'addestramento e la validazione del modello.\\
        Trattandosi di dati particolarmente sensibili, il numero di dataset presenti online non è molto alto. Tuttavia, un \href{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}{dataset} in particolare, sembra essere particolarmente rilevante per gli scopi di questo progetto.

    \subsection{Data Description}
        Procedo con lo studio dei dati selezionati per averne una maggiore comprensione. Il dataset individuato, realizzato e rilasciato dai Cardholders europei, contiene l'insieme delle transazioni effettuate da carte di credito in due giorni del settembre 2013.\\
        I dati presenti sono tutti numerici e sono il risultato di una trasformazione PCA. La Principal Component Analysis è una tecnica di riduzione della dimensionalità di un dataset, le cui variabili vengono trasformate in un nuovo set di variabili, che sono combinazioni lineari delle variabili originali (il risultato finale è, dunque, un nuovo dataset).\\
        Chiaramente, per motivi di riservatezza, non sono state fornite le caratteristiche originali, di cui non sappiamo nulla. Le componenti principali ottenute con PCA sono le features V1, V2, ..., V28. Le uniche caratteristiche che non sono state sottoposte alla trasformazione PCA sono \textit{Time} e \textit{Amount}: \textit{Time} rappresenta il tempo, in secondi, trascorso tra ogni transazione e la prima transazione del dataset, mentre \textit{Amount} rappresenta l'ammontare, in USD, della transazione. La variabile \textit{Class}, infine, è la nostra variabile dipendente che assume valore 0, se la transazione è autorizzata (dunque lecita), oppure 1, se la transazione risulta essere fraudolenta.\\

    \subsection{Data Quality}
        La prima cosa che controllo è se il dataset è bilanciato o meno. Su un totale di 284807 transazioni, soltanto 492 di esse risultano essere fraudolente: si tratta dello 0.172\% del totale. Pertanto, il dataset risulta essere \textit{altamente sbilanciato}, come osservabile nel seguente grafico:
        \begin{figure}[H]
            \centering
            \includegraphics[width=.6\textwidth]{img/classDistribution.png}
            \caption[short]{}
        \end{figure}

        Non ci sono valori \textit{null} all'interno del dataset e l'ammontare delle transazioni fraudolente risulta essere, in media, di 88,2 USD. Media che risulta essere più bassa di quella delle transazioni lecite che, invece, ammonta a 122,2 USD. Altro aspetto da tenere in considerazione è che le features da V1 a V28 sono già state scalate in quanto sottoposte a trasformazione PCA.\\
        L'utilizzo di un dataset così sbilanciato, porterebbe alla costruzione di un modello altamente impreciso e che, probabilmente, si \textit{adatterà troppo (overfitting)} al problema, in quanto andrebbe ad assumere la maggior parte delle transazioni come non fraudolente, siccome queste costituiscono la stragrande maggioranza dei dati.\\
        Un'altra delle conseguenze relative all'utilizzo di un dataset sbilanciato, è la mancata comprensione della correlazione tra le varie caratteristiche.

\section{Data Preparation}
    Dopo aver studiato il dataset e averne ottenuto un certo grado di comprensione, passo alla fase di \textit{Data Preparation}, che produrrà il dataset pronto per essere dato in input al modello.
    \subsection{Data Cleaning}
        La fase di \textit{Data Cleaning}, è la fase in cui si vanno a gestire dati mancanti o rumorosi.
        Il dataset considerato non presenta alcun valore \textit{null} o rumoroso, per cui si può passare alla fase di \textit{Feature Scaling}.

    \subsection{Feature Scaling}
        Nella fase di \textit{Feature Scaling} si utilizzando apposite tecniche per scalare o normalizzare dati con valori particolarmente diversi tra loro. Infatti, addestrare un modello con un dataset che presenta valori per una determinata caratteristica particolarmente distanti dagli altri, potrebbe ingannare il modello il quale andrebbe a sovrastimare o sottostimare l'importanza di una caratteristica. Nel nostro caso, \textit{Amount} e \textit{Time} sono le uniche due caratteristiche che presentano valori diversi dagli altri poiché questi ultimi, come già illustrato, sono già stati scalati.\\
        Per la selezione dello \textit{scaler} più appropriato per i dati in questione, procedo con l'osservazione della loro distribuzione.
        \begin{figure}[H]
            \centering
            \includegraphics[width=.9\textwidth]{img/valuesDistribution.png}
            \caption[short]{Distribuzioni delle variabili da scalare}
        \end{figure}
        La distribuzione dei valori di \textit{Time} mostra una certa periodicità o struttura ciclica, suggerendo che alcune ore del giorno potrebbero essere più propense a transazioni rispetto ad altre.\\
        La distribuzione dei valori di \textit{Amount}, invece, risulta essere \textit{fortemente asimmetrica} a destra: la maggior parte delle transazioni risultano essere caratterizzate da importi relativamente bassi, mentre poche sono quelle con importi più alti (queste ultime transazioni rappresentano degli \textit{outliers}, cioè dei valori anomali rispetto al resto della distribuzione).\\
        La tecnica di scaling che risulta essere più appropriata per gestire una distribuzione fortemente asimmetrica è quella del \textit{Robust Scaling}.\\
        La tecnica di Robust Scaling è quella più resistente agli outliers poiché utilizza \textit{mediana} e \textit{IQR} (Range Interquartile). Infatti, una tecnica che utilizza la media (e.g. Standard Scaling) risulta essere molto sensibile agli outliers, in quanto essi ne causano una notevole variazione. Invece, la mediana non dipende da ogni singolo valore, dunque è meno soggetta ad alterazioni in presenza di outliers.\\
        Di seguito la formula di Roubust Scaling:\\
        \begin{center}
            $\text{Robust Scaling}(x) = \frac{x - \text{Mediana}(x)}{\text{IQR}(x)}$
        \end{center}

    \subsection{Feature Selection}
        Procedo con la fase di Feature Selection, nella quale, appunto, vado ad individuare e selezionare le caratteristiche con potenza predittiva più alta, scartando quelle con potenza predittiva più bassa che potrebbero rendere il futuro modello più impreciso o peggiorarne le performances.
        \subsubsection{Correlation Matrix}
            La tecnica che utilizzo è nota come \textbf{Matrice di Correlazione}. Tale matrice mostra i \textit{coefficienti di correlazione} tra coppie di variabili. Tali coefficienti possono assumere valori racchiusi nell'intervallo $[-1, 1]$. Valori vicini a 1 indicano che le due variabili sono \textbf{correlate positivamente}, valori vicino a -1 indicano che le due variabili sono \textbf{correlate negativamente}, valori vicini a 0, infine, indicano che tra le due variabili \textbf{non intercorre alcuna relazione}.
            \begin{figure}[H]
                \centering
                \includegraphics[width=.6\textwidth]{img/correlationMatrix.png}
                \caption[short]{Matrice di correlazione}
            \end{figure}
            Il numero delle features è elevato, dunque la lettura del grado di  correlazione tra loro non risulta essere particolarmente agevole. Per tale ragione, decido di andare a generare un secondo grafico (diviso, per comodità, in due parti) il quale mostra soltanto le correlazioni di ciascuna caratteristica con la variabile target.
            \begin{figure}[H]
                \centering
                \includegraphics[width=.3\textwidth]{img/correlationFigure.png}
                \caption[short]{Correlazione tra features e variabile target}
            \end{figure}
            Le caratteristiche che andrò a scartare in seguito, sono quelle che hanno un grado di correlazione con la feature \textit{Class} compreso nell'intervallo [-0.1, 0.1]. Quelle che manterrò sono, invece, la V4 e la V11, le quali sono correlate positivamente con \textit{Class}, e V7, V3, V16, V10, V12, V14, V17, che sono correlate negativamente con la variabile target.



    \subsection{Data Balancing}
        Come analizzato nella fase di Data Understanding, il dataset risulta essere altamente \textit{unbalanced}. Addestrare un modello su un dataset del genere, comporterebbe un elevato rischio di overfitting. Per ridurlo, è necessario andare a bilanciare il dataset.
        \subsubsection{Data Splitting}
            Prima di procedere, divido il dataset in \textit{training set} e \textit{test set}. E' cruciale effettuare questa divisione prima di effettuare qualsiasi operazione di sampling per evitare di incappare in problemi di \textbf{data leakage}. Infatti, sarebbe un errore testare e valutare il modello su dei dati bilanciati poiché questi dovrebbero rispecchiare la realtà quanto più è possibile. Se così non fosse, infatti, il modello effettuerebbe predizioni accurate in fase di addestramento, ma non in fase di rilascio.\\
            Dunque, innanzitutto separo le features dalla variabile target. Per fare questo, utilizzo il metodo \textit{iloc} della libreria pandas: in particolare, nella prima istruzione seleziono tutte le righe, tramite \textit{:}, delle prime nove colonne (da 0 a 8) e poi, tramite \textit{values}, converto in un array numpy; nella seconda istruzione, invece, seleziono tutte le righe della decima colonna (di indice 9)  del dataframe.
            \begin{verbatim}
                # Separating features and target
                X = df1.iloc[:,:9].values
                y = df1.iloc[:,9].values
            \end{verbatim}
            Successivamente, divido il dataset assegnando il 30\% dei dati al test set, mentre il restante 70\% di essi costituiranno i dati di addestramento.
        \subsubsection{Sampling Techniqe Selection}
            Per bilanciare un dataset, possiamo utilizzare due tecniche:
            \begin{itemize}
                \item \textbf{undersampling},
                \item \textbf{oversampling}.
            \end{itemize}
            La tecnica di undersampling, consiste nella rimozione di samples dalla classe di maggioranza che, nel nostro caso, è quella delle transazioni non fraudolente. Tuttavia, scegliendo di utilizzare questa tecnica, mi ritroverei con un dataset eccessivamente piccolo, che vedrebbe perdute la maggior parte delle informazioni, avendo soltanto, come osservato in precedenza, 492 samples della classe di minoranza (Frauds).\\
            Allo stesso tempo, la tecnica di oversampling, che, invece, consiste nella generazione casuale di nuovi samples della classe di minoranza, potrebbe condurre il modello verso l'overfitting, in quanto, ancora, il numero dei samples della classe di minoranza e' eccessivamente basso.\\
            Per tale ragione, decido di utilizzare una tecnica nota come \textbf{Synthetic Minority Oversampling Tecnhique} (\textbf{SMOTE}).
            SMOTE e' una tecnica di oversampling che, invece di creare copie di istanze esistenti della classe di minoranza, genera nuove istanze (dette istanze sintetiche) attraverso l'interpolazion, dunque mi permette di mitigare il rischio di overfitting. Di seguito illustrato il funzionamento:
            \begin{enumerate}[i.]
                \item Seleziona una istanza random all'interno della classe di minoranza
                \item Identifica k vicini per il dato appena selezionato
                \item Seleziona uno di questi vicini per il nuovo dato da creare
                \item Calcola la distanza vettoriale tra il punto e il vicino selezionati
                \item Moltiplica tale distanza per un numero casuale compreso tra 0 e 1
            \end{enumerate}
            \begin{figure}[H]
                \centering
                \includegraphics[width=.45\textwidth]{img/SMOTE.png}
                \caption[short]{Meccanismo SMOTE}
            \end{figure}
            Prima dell'applicazione di SMOTE, all'interno dei dati di training si hanno:
            \begin{itemize}
                \item N. transazioni fraudolente: 345
                \item N. transazioni non fraudolente: 199019
            \end{itemize}
            Dopo l'applicazione di SMOTE, invece, si hanno:
            \begin{itemize}
                \item N. transazioni fraudolente: 199019
                \item N. transazioni non fraudolente: 199019
            \end{itemize}
\section{Modeling}
\end{document}